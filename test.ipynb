{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis completed. Results saved in 'analysis_results.csv'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from misinformation_detector import MisinformationDetector\n",
    "\n",
    "# ============================ STEP 1: CLEAN THE DATA ============================\n",
    "\n",
    "# Define dataset path (change to \"test.tsv\" or \"valid.tsv\" as needed)\n",
    "dataset_path = \"data/combined_dataset.tsv\"\n",
    "\n",
    "# Column names in the LIAR dataset\n",
    "column_names = [\n",
    "    \"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"job_title\", \"state\",\n",
    "    \"party_affiliation\", \"barely_true_counts\", \"false_counts\", \"half_true_counts\",\n",
    "    \"mostly_true_counts\", \"pants_on_fire_counts\", \"context\"\n",
    "]\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(dataset_path, sep='\\t', names=column_names, index_col=False)\n",
    "\n",
    "# Remove ambiguous labels (keeping only \"true\", \"false\", \"pants-fire\")\n",
    "df = df[df[\"label\"].isin([\"true\", \"false\", \"pants-fire\"])]\n",
    "\n",
    "# Remove statements with less than 5 words (not informative)\n",
    "df = df[df[\"statement\"].str.split().str.len() >= 7]\n",
    "\n",
    "# Remove statements with missing speakers\n",
    "df = df.dropna(subset=[\"speaker\"])\n",
    "\n",
    "# ============================ STEP 2: RUN MISINFORMATION DETECTION ============================\n",
    "\n",
    "# Load cleaned dataset\n",
    "test_df = df\n",
    "\n",
    "# Convert labels to binary misinformation classification\n",
    "def convert_label(label):\n",
    "    misinformation_labels = [\"false\", \"pants-fire\"]  # \"true\" is considered not misinformation\n",
    "    return label in misinformation_labels\n",
    "\n",
    "test_df[\"contains_misinformation\"] = test_df[\"label\"].apply(convert_label)\n",
    "test_df = test_df[[\"statement\", \"contains_misinformation\"]]\n",
    "\n",
    "# Initialize the Misinformation Detector\n",
    "detector = MisinformationDetector()\n",
    "\n",
    "# Sample a subset of test data for analysis (to avoid excessive API calls)\n",
    "test_sample = test_df.sample(n=100, random_state=53)\n",
    "\n",
    "# Store results\n",
    "analysis_results = []\n",
    "\n",
    "# Run analysis on the test dataset\n",
    "for index, row in test_sample.iterrows():\n",
    "    statement = row[\"statement\"]\n",
    "    ground_truth = row[\"contains_misinformation\"]\n",
    "\n",
    "    # Run misinformation detection model\n",
    "    result = detector.analyze_text(statement)\n",
    "\n",
    "    # Store results\n",
    "    analysis_results.append({\n",
    "        \"statement\": statement,\n",
    "        \"ground_truth\": ground_truth,\n",
    "        \"model_prediction\": result[\"contains_misinformation\"],\n",
    "        \"confidence_score\": result[\"confidence_score\"],\n",
    "        \"detected_criteria\": result[\"detected_criteria\"],\n",
    "        \"explanation\": result[\"explanation\"],\n",
    "        \"prompt_for_context\": result[\"prompt_for_context\"]\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame and save\n",
    "results_df = pd.DataFrame(analysis_results)\n",
    "results_df.to_csv(\"analysis_results.csv\", index=False)\n",
    "\n",
    "print(\"Analysis completed. Results saved in 'analysis_results.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>model_prediction</th>\n",
       "      <th>confidence_score</th>\n",
       "      <th>detected_criteria</th>\n",
       "      <th>explanation</th>\n",
       "      <th>prompt_for_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The actual deportations from the interior of t...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>[1]</td>\n",
       "      <td>The statement claims that deportations from th...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Martin Luther King Jr. was a Republican!</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>The claim that Martin Luther King Jr. was a Re...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are three times more likely to be able to ...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.8</td>\n",
       "      <td>[]</td>\n",
       "      <td>The statement appears to be a factual claim ba...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Under Donald Trumps tax plan, 51 percent of si...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.8</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>The statement makes a specific claim about the...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Says the Patient Protection and Affordable Car...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.8</td>\n",
       "      <td>[1, 2, 5]</td>\n",
       "      <td>The text makes a claim about the Patient Prote...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           statement  ground_truth  \\\n",
       "0  The actual deportations from the interior of t...         False   \n",
       "1           Martin Luther King Jr. was a Republican!          True   \n",
       "2  You are three times more likely to be able to ...          True   \n",
       "3  Under Donald Trumps tax plan, 51 percent of si...         False   \n",
       "4  Says the Patient Protection and Affordable Car...          True   \n",
       "\n",
       "   model_prediction  confidence_score detected_criteria  \\\n",
       "0              True               0.7               [1]   \n",
       "1              True               0.9            [1, 2]   \n",
       "2             False               0.8                []   \n",
       "3              True               0.8            [1, 2]   \n",
       "4              True               0.8         [1, 2, 5]   \n",
       "\n",
       "                                         explanation  prompt_for_context  \n",
       "0  The statement claims that deportations from th...               False  \n",
       "1  The claim that Martin Luther King Jr. was a Re...               False  \n",
       "2  The statement appears to be a factual claim ba...                True  \n",
       "3  The statement makes a specific claim about the...                True  \n",
       "4  The text makes a claim about the Patient Prote...                True  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"analysis_results.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misinformation Detection Benchmark Results:\n",
      "Accuracy: 0.6100\n",
      "Precision: 0.6585\n",
      "Recall: 0.8308\n",
      "F1-score: 0.7347\n",
      "ROC-AUC Score: 0.6057\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Load the analysis results\n",
    "df = pd.read_csv(\"analysis_results.csv\")\n",
    "\n",
    "# Extract ground truth and model predictions\n",
    "y_true = df[\"ground_truth\"].astype(int)  # Convert boolean to int (0 or 1)\n",
    "y_pred = df[\"model_prediction\"].astype(int)  # Convert boolean to int (0 or 1)\n",
    "y_scores = df[\"confidence_score\"]  # Confidence scores from the model\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "roc_auc = roc_auc_score(y_true, y_scores)\n",
    "\n",
    "# Print results\n",
    "print(\"Misinformation Detection Benchmark Results:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Misinformation Detection Benchmark Results (Threshold=0.75):\n",
      "‚úÖ Accuracy: 0.6700\n",
      "üéØ Precision: 0.6905\n",
      "üìà Recall: 0.8923\n",
      "‚öñÔ∏è  F1-score: 0.7785\n",
      "üìä ROC-AUC Score: 0.6057\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Load analysis results\n",
    "df = pd.read_csv(\"analysis_results.csv\")\n",
    "\n",
    "# Extract ground truth and confidence scores\n",
    "y_true = df[\"ground_truth\"].astype(int)\n",
    "y_scores = df[\"confidence_score\"]  # Model‚Äôs confidence score\n",
    "\n",
    "# üîπ Set a higher threshold for classifying as misinformation\n",
    "threshold = 0.75  # Change this value (0.7, 0.8, etc.) to test different thresholds\n",
    "y_pred = (y_scores >= threshold).astype(int)\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "roc_auc = roc_auc_score(y_true, y_scores)\n",
    "\n",
    "# Print updated benchmark results\n",
    "print(f\"üîç Misinformation Detection Benchmark Results (Threshold={threshold}):\")\n",
    "print(f\"‚úÖ Accuracy: {accuracy:.4f}\")\n",
    "print(f\"üéØ Precision: {precision:.4f}\")\n",
    "print(f\"üìà Recall: {recall:.4f}\")\n",
    "print(f\"‚öñÔ∏è  F1-score: {f1:.4f}\")\n",
    "print(f\"üìä ROC-AUC Score: {roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç False positives saved to 'false_positives.csv'.\n",
      "üîç False negatives saved to 'false_negatives.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Find misclassified cases\n",
    "df[\"predicted\"] = y_pred\n",
    "\n",
    "# üîπ False Positives: Statements incorrectly flagged as misinformation\n",
    "false_positives = df[(df[\"ground_truth\"] == 0) & (df[\"predicted\"] == 1)]\n",
    "\n",
    "# üîπ False Negatives: Misinformation that was missed\n",
    "false_negatives = df[(df[\"ground_truth\"] == 1) & (df[\"predicted\"] == 0)]\n",
    "\n",
    "# Save misclassified cases for review\n",
    "false_positives.to_csv(\"false_positives.csv\", index=False)\n",
    "false_negatives.to_csv(\"false_negatives.csv\", index=False)\n",
    "\n",
    "print(\"üîç False positives saved to 'false_positives.csv'.\")\n",
    "print(\"üîç False negatives saved to 'false_negatives.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Confidence scores normalized and saved as 'normalized_analysis_results.csv'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Normalize confidence scores to 0-1 range\n",
    "scaler = MinMaxScaler()\n",
    "df[\"confidence_score\"] = scaler.fit_transform(df[[\"confidence_score\"]])\n",
    "\n",
    "# Save the updated analysis results\n",
    "df.to_csv(\"normalized_analysis_results.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Confidence scores normalized and saved as 'normalized_analysis_results.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Benchmark Results (Threshold=0.6):\n",
      "‚úÖ Accuracy: 0.6400\n",
      "üéØ Precision: 0.6465\n",
      "üìà Recall: 0.9846\n",
      "‚öñÔ∏è  F1-score: 0.7805\n",
      "üìä ROC-AUC Score: 0.6057\n",
      "\n",
      "üîç Benchmark Results (Threshold=0.7):\n",
      "‚úÖ Accuracy: 0.6400\n",
      "üéØ Precision: 0.6465\n",
      "üìà Recall: 0.9846\n",
      "‚öñÔ∏è  F1-score: 0.7805\n",
      "üìä ROC-AUC Score: 0.6057\n",
      "\n",
      "üîç Benchmark Results (Threshold=0.75):\n",
      "‚úÖ Accuracy: 0.6700\n",
      "üéØ Precision: 0.6905\n",
      "üìà Recall: 0.8923\n",
      "‚öñÔ∏è  F1-score: 0.7785\n",
      "üìä ROC-AUC Score: 0.6057\n",
      "\n",
      "üîç Benchmark Results (Threshold=0.8):\n",
      "‚úÖ Accuracy: 0.6700\n",
      "üéØ Precision: 0.6905\n",
      "üìà Recall: 0.8923\n",
      "‚öñÔ∏è  F1-score: 0.7785\n",
      "üìä ROC-AUC Score: 0.6057\n",
      "\n",
      "‚úÖ Threshold tuning results saved as 'threshold_tuning_results.csv'.\n",
      "\n",
      "üîç Misclassified cases saved:\n",
      "‚ö†Ô∏è  False positives saved to 'false_positives.csv'.\n",
      "‚ö†Ô∏è  False negatives saved to 'false_negatives.csv'.\n",
      "\n",
      "‚úÖ Confidence scores normalized and saved as 'normalized_analysis_results.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# ===================== STEP 1: LOAD ANALYSIS RESULTS =====================\n",
    "\n",
    "# Load analysis results\n",
    "df = pd.read_csv(\"analysis_results.csv\")\n",
    "\n",
    "# Extract ground truth and confidence scores\n",
    "y_true = df[\"ground_truth\"].astype(int)\n",
    "y_scores = df[\"confidence_score\"]  # Model‚Äôs confidence scores\n",
    "\n",
    "# ===================== STEP 2: THRESHOLD TUNING =====================\n",
    "\n",
    "# üîπ Try different thresholds (adjust these values)\n",
    "thresholds = [0.6, 0.7, 0.75, 0.8]\n",
    "\n",
    "# Store benchmark results for each threshold\n",
    "benchmark_results = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Apply threshold to make predictions\n",
    "    y_pred = (y_scores >= threshold).astype(int)\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)  # Avoid division errors\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_scores)\n",
    "\n",
    "    # Store results\n",
    "    benchmark_results.append({\n",
    "        \"Threshold\": threshold,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-score\": f1,\n",
    "        \"ROC-AUC\": roc_auc\n",
    "    })\n",
    "\n",
    "    print(f\"\\nüîç Benchmark Results (Threshold={threshold}):\")\n",
    "    print(f\"‚úÖ Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"üéØ Precision: {precision:.4f}\")\n",
    "    print(f\"üìà Recall: {recall:.4f}\")\n",
    "    print(f\"‚öñÔ∏è  F1-score: {f1:.4f}\")\n",
    "    print(f\"üìä ROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "# Save threshold tuning results\n",
    "pd.DataFrame(benchmark_results).to_csv(\"threshold_tuning_results.csv\", index=False)\n",
    "print(\"\\n‚úÖ Threshold tuning results saved as 'threshold_tuning_results.csv'.\")\n",
    "\n",
    "# ===================== STEP 3: FIND MISCLASSIFIED CASES =====================\n",
    "\n",
    "# Apply best threshold (adjust if needed based on previous results)\n",
    "best_threshold = 0.75\n",
    "df[\"predicted\"] = (df[\"confidence_score\"] >= best_threshold).astype(int)\n",
    "\n",
    "# üîπ False Positives: Statements incorrectly flagged as misinformation\n",
    "false_positives = df[(df[\"ground_truth\"] == 0) & (df[\"predicted\"] == 1)]\n",
    "\n",
    "# üîπ False Negatives: Misinformation that was missed\n",
    "false_negatives = df[(df[\"ground_truth\"] == 1) & (df[\"predicted\"] == 0)]\n",
    "\n",
    "# Save misclassified cases\n",
    "false_positives.to_csv(\"false_positives.csv\", index=False)\n",
    "false_negatives.to_csv(\"false_negatives.csv\", index=False)\n",
    "\n",
    "print(\"\\nüîç Misclassified cases saved:\")\n",
    "print(\"‚ö†Ô∏è  False positives saved to 'false_positives.csv'.\")\n",
    "print(\"‚ö†Ô∏è  False negatives saved to 'false_negatives.csv'.\")\n",
    "\n",
    "# ===================== STEP 4: NORMALIZE CONFIDENCE SCORES =====================\n",
    "\n",
    "# Normalize confidence scores to 0-1 range\n",
    "scaler = MinMaxScaler()\n",
    "df[\"normalized_confidence\"] = scaler.fit_transform(df[[\"confidence_score\"]])\n",
    "\n",
    "# Save updated dataset\n",
    "df.to_csv(\"normalized_analysis_results.csv\", index=False)\n",
    "print(\"\\n‚úÖ Confidence scores normalized and saved as 'normalized_analysis_results.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
